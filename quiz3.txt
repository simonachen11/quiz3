from newspaper import Article

url = 'https://www.sciencedaily.com/releases/2021/08/210811162816.htm'
article = Article(url)
article.download()
article.parse()
article.text
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation
from heapq import nlargest

def summarize(text, per):
    nlp = spacy.load('en_core_web_sm')
    doc= nlp(text)
    tokens=[token.text for token in doc]
    word_frequencies={}
    for word in doc:
        if word.text.lower() not in list(STOP_WORDS):
            if word.text.lower() not in punctuation:
                if word.text not in word_frequencies.keys():
                    word_frequencies[word.text] = 1
                else:
                    word_frequencies[word.text] += 1
    max_frequency=max(word_frequencies.values())
    for word in word_frequencies.keys():
        word_frequencies[word]=word_frequencies[word]/max_frequency
    sentence_tokens= [sent for sent in doc.sents]
    sentence_scores = {}
    for sent in sentence_tokens:
        for word in sent:
            if word.text.lower() in word_frequencies.keys():
                if sent not in sentence_scores.keys():                            
                    sentence_scores[sent]=word_frequencies[word.text.lower()]
                else:
                    sentence_scores[sent]+=word_frequencies[word.text.lower()]
    select_length=int(len(sentence_tokens)*per)
    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)
    final_summary=[word.text for word in summary]
    summary=''.join(final_summary)
    return summary
    
    
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'
def preprocess(sent):
    sent = nltk.word_tokenize(sent)
    sent = nltk.pos_tag(sent)
    return sent
    
 sent = preprocess(ex)

from nltk.chunk import conlltags2tree, tree2conlltags
from pprint import pprint
iob_tagged = tree2conlltags(cs)
pprint(iob_tagged)

doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')
pprint([(X.text, X.label_) for X in doc.ents])

from bs4 import BeautifulSoup
import requests
import re
def url_to_string(url):
    res = requests.get(url)
    html = res.text
    soup = BeautifulSoup(html, 'html5lib')
    for script in soup(["script", "style", 'aside']):
        script.extract()
    return " ".join(re.split(r'[\n\t]+', soup.get_text()))
ny_bb = url_to_string('https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news')
article = nlp(ny_bb)
len(article.ents)

>>> from pprint import pprint

>>> text = """
... For some quick analysis, creating a corpus could be overkill.
... If all you need is a word list,
... there are simpler ways to achieve that goal."""
>>> pprint(nltk.word_tokenize(text), width=79, compact=True)
['For', 'some', 'quick', 'analysis', ',', 'creating', 'a', 'corpus', 'could',
 'be', 'overkill', '.', 'If', 'all', 'you', 'need', 'is', 'a', 'word', 'list',
 ',', 'there', 'are', 'simpler', 'ways', 'to', 'achieve', 'that', 'goal', '.']
 words: list[str] = nltk.word_tokenize(text)
fd = nltk.FreqDist(words)
>>> fd.most_common(3)
[('must', 1568), ('people', 1291), ('world', 1128)]
>>> fd.tabulate(3)
  must people  world
  1568   1291   1128
  
>>> text = nltk.Text(nltk.corpus.state_union.words())
>>> text.concordance("america", lines=5)
Displaying 5 of 1079 matches:
 would want us to do . That is what America will do . So much blood has already
ay , the entire world is looking to America for enlightened leadership to peace
beyond any shadow of a doubt , that America will continue the fight for freedom
 to make complete victory certain , America will never become a party to any pl
nly in law and in justice . Here in America , we have labored long and hard to
























{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f37d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Huggingface interface\n",
    "# - https://huggingface.co/transformers/notebooks.html\n",
    "# - https://github.com/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb\n",
    "# - More examples with LLMs at: https://github.com/biplav-s/course-tai/blob/3a37536b00a0b386d32cb29da61b1ce68f72cfdb/sample-code/l13-l16-supervised-text/l15-langmodel-commontasks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa4b6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers installation, if needed\n",
    "#! pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef24d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Huggingface pipeline abstraction for common tasks\n",
    "# - Pipelines: https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "# - Sentiment based handling: https://huggingface.co/blog/sentiment-analysis-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f19d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model used is - \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b5013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"She is angry most of the time, but she makes me happy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad193f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n"
     ]
    }
   ],
   "source": [
    "# Now run to see sentiments\n",
    "results = classifier(data)\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "674241ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more general data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ae9ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [\"this is good\",\n",
    "        \"this is not bad\",\n",
    "        \"this is bad bad bad\",\n",
    "        \"this is too good\",\n",
    "         \"this is too bad\",\n",
    "         \"this is not bad\",\n",
    "        \"No one did a bad action\", \n",
    "         \"Jamil did a bad action\",         \n",
    "         \"John did a bad action\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "726f2a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a convenient function to run on a list of inputs \n",
    "def sentimClassifyPrint(alist):\n",
    "    results = classifier(alist)\n",
    "    for result in results:\n",
    "        print(alist[results.index(result)] + f\" <-> label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "405a47d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is good <-> label: POSITIVE, with score: 0.9998\n",
      "this is not bad <-> label: POSITIVE, with score: 0.9997\n",
      "this is bad bad bad <-> label: NEGATIVE, with score: 0.9998\n",
      "this is too good <-> label: POSITIVE, with score: 0.9997\n",
      "this is too bad <-> label: NEGATIVE, with score: 0.9998\n",
      "this is not bad <-> label: POSITIVE, with score: 0.9997\n",
      "No one did a bad action <-> label: POSITIVE, with score: 0.9989\n",
      "Jamil did a bad action <-> label: NEGATIVE, with score: 0.9982\n",
      "John did a bad action <-> label: NEGATIVE, with score: 0.9989\n"
     ]
    }
   ],
   "source": [
    "sentimClassifyPrint(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd5f9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now some data from water domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e18f8d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = [\"NSDWRs (or secondary standards) are non-enforceable guidelines regulating contaminants that may cause cosmetic effects (such as skin or tooth discoloration) or aesthetic effects (such as taste, odor, or color) in drinking water.\",\n",
    "        \" EPA recommends secondary standards to water systems but does not require systems to comply with the standard. \",\n",
    "        \"However, states may choose to adopt them as enforceable standards.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14484d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSDWRs (or secondary standards) are non-enforceable guidelines regulating contaminants that may cause cosmetic effects (such as skin or tooth discoloration) or aesthetic effects (such as taste, odor, or color) in drinking water. <-> label: NEGATIVE, with score: 0.9922\n",
      " EPA recommends secondary standards to water systems but does not require systems to comply with the standard.  <-> label: POSITIVE, with score: 0.9548\n",
      "However, states may choose to adopt them as enforceable standards. <-> label: NEGATIVE, with score: 0.9046\n"
     ]
    }
   ],
   "source": [
    "sentimClassifyPrint(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "484c70da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many statements above are more neutral than positive or negative but the model is confident"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e7824a",
   "metadata": {},
   "source": [
    "# Other common sentiment models to use\n",
    "'''\n",
    "- From: https://github.com/huggingface/notebooks/blob/main/transformers_doc/quicktour.ipynb\n",
    "    1. Twitter-roberta-base-sentiment is a roBERTa model trained on ~58M tweets \n",
    "          and fine-tuned for sentiment analysis. Fine-tuning is the process of taking a \n",
    "          pre-trained large language model (e.g. roBERTa in this case) and then tweaking it \n",
    "          with additional training data to make it perform a second similar task (e.g. \n",
    "          sentiment analysis).\n",
    "    2. Bert-base-multilingual-uncased-sentiment is a model fine-tuned for \n",
    "          sentiment analysis on product reviews in six languages: English, Dutch, German, \n",
    "          French, Spanish and Italian.\n",
    "    3. Distilbert-base-uncased-emotion is a model fine-tuned for detecting emotions \n",
    "          in texts, including sadness, joy, love, anger, fear and surprise.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab2d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
